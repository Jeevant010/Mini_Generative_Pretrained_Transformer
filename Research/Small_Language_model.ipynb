{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c82f3e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eed5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "534421f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cpu'}\n"
     ]
    }
   ],
   "source": [
    "device = {\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'mps' if torch.backends.mps.is_available()\n",
    "    else 'cpu'  \n",
    "}   \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a090e7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.cuda.is_available())  \u001b[38;5;66;03m# Should print True\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Should show your GPU name\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\LLM\\Lib\\site-packages\\torch\\cuda\\__init__.py:584\u001b[39m, in \u001b[36mget_device_name\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_name\u001b[39m(device: \u001b[33m\"\u001b[39m\u001b[33mDevice\u001b[39m\u001b[33m\"\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    573\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[32m    574\u001b[39m \n\u001b[32m    575\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m \u001b[33;03m        str: the name of the device\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.name\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\LLM\\Lib\\site-packages\\torch\\cuda\\__init__.py:616\u001b[39m, in \u001b[36mget_device_properties\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    604\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_properties\u001b[39m(device: \u001b[33m\"\u001b[39m\u001b[33mDevice\u001b[39m\u001b[33m\"\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m) -> _CudaDeviceProperties:\n\u001b[32m    605\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[32m    606\u001b[39m \n\u001b[32m    607\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    614\u001b[39m \u001b[33;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[32m    617\u001b[39m     device = _get_device_index(device, optional=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m device < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device >= device_count():\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\LLM\\Lib\\site-packages\\torch\\cuda\\__init__.py:403\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    400\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    401\u001b[39m     )\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True\n",
    "print(torch.cuda.get_device_name(0))  # Should show your GPU name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "760ccf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0+cu124\n",
      "CUDA version: 12.4\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f4b5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "batch_size = 8\n",
    "block_size = 32\n",
    "max_iters = 1000\n",
    "learning_rate = 1e-4\n",
    "eval_iters = 100\n",
    "n_embd = 128\n",
    "n_head = 8\n",
    "n_layer = 1\n",
    "dropout = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7c9bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = \"\"\n",
    "with open('../wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c4115",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vocab.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m chars = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvocab.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m     f.read()\n\u001b[32m      4\u001b[39m     chars = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(text)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jeeva\\.conda\\envs\\LLM\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'vocab.txt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a72f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('../wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "472e18c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vocab.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m chars = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvocab.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m     f.read()\n\u001b[32m      4\u001b[39m     chars = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(text)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jeeva\\.conda\\envs\\LLM\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'vocab.txt'"
     ]
    }
   ],
   "source": [
    "chars = \"\"\n",
    "with open(\"vocab.txt\" , 'r', encoding='utf-8') as f:\n",
    "    f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "    \n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79465ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
      "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
      "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
      "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
      "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
     ]
    }
   ],
   "source": [
    "# Suppose chars is a list or string of unique characters\n",
    "# e.g., chars = sorted(list(set(text)))\n",
    "\n",
    "string_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_string = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode: string -> list of integers\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "\n",
    "# Decode: list of integers -> string\n",
    "decode = lambda l: ''.join(int_to_string[i] for i in l)\n",
    "\n",
    "# Convert encoded data to a tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "163e91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(split):\n",
    "    filename = \"../wizard_of_oz.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "            \n",
    "            # Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size * batch_size-1)\n",
    "            \n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            \n",
    "            # Train and test splits\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long )\n",
    "            \n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "310374eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split) :\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i : i + batch_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + batch_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module ) :\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size) : \n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False )\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False )\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False )\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size )))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input of size (batch ,time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)    # (B, T , hs)\n",
    "        q = self.query(x)  # (B, hs, T)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, 1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[ : T, : T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        #perform the weighted aggregation of the values\n",
    "        v = self.values(x) # (B, T, hs)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2092e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size) :\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(Head(head_size) for _ in range(num_heads))\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) \n",
    "        #  (B, T, F) -> ( B, T , [h1, h1, h1, h1, h2, h2, h2, h2, h3 ... ])\n",
    "        out = self,dropout(self.proj(out))\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed y a non-linearity \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd) : \n",
    "        super().__init__()\n",
    "        self.net = nn.Sequencial(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac6852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module) :\n",
    "    \"\"\"\" Transformer block : communication followed by computation \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        #n_embd: embedding dimension, n_head : number of heads we'd like to be \n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1( x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2( x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4c8dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m             index = torch.cat((index, index_next), dim=\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# (B, t + 1)\u001b[39;00m\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m index\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m model = \u001b[43mGPTLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m m = model.to(device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mGPTLanguageModel.__init__\u001b[39m\u001b[34m(self, vocab_size)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size) : \n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_embedding_table\u001b[49m = nn.Embedding(vocab_size, n_embd)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.position_embedding_table = nn.Embedding(block_size, n_embd)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mself\u001b[39m.blocks = nn.Sequencial(* [Block(n_embd, n_head=n_head) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layer)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jeeva\\.conda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1965\u001b[39m, in \u001b[36mModule.__setattr__\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   1963\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Module):\n\u001b[32m   1964\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1965\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1966\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcannot assign module before Module.__init__() call\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1967\u001b[39m         )\n\u001b[32m   1968\u001b[39m     remove_from(\n\u001b[32m   1969\u001b[39m         \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m,\n\u001b[32m   1970\u001b[39m         \u001b[38;5;28mself\u001b[39m._parameters,\n\u001b[32m   1971\u001b[39m         \u001b[38;5;28mself\u001b[39m._buffers,\n\u001b[32m   1972\u001b[39m         \u001b[38;5;28mself\u001b[39m._non_persistent_buffers_set,\n\u001b[32m   1973\u001b[39m     )\n\u001b[32m   1974\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m _global_module_registration_hooks.values():\n",
      "\u001b[31mAttributeError\u001b[39m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size) : \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequencial(* [Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def __init_weights(self, module) :\n",
    "        if isinstance(module, nn.Linear) :\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None :\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding) :\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, index, targets=None) : \n",
    "        B, T = index.shape\n",
    "        \n",
    "        # index and targets both B, T tesors of integers\n",
    "        tok_emb = self.token_embedding_table(index)\n",
    "        pos_emb = self.position_embedding_table(torch.arrange(T, device=device)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x) # (B, T , C)\n",
    "        x = self.ln_f(x) # (B, T , C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None : \n",
    "            loss = None\n",
    "        else : \n",
    "            B, T , C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens ) :\n",
    "        #index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens) : \n",
    "            # Crop index to the Last block size tokens\n",
    "            index_cond = index[ : , -block_size : ]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            #focus only on the loast time step\n",
    "            probs = logits[ : , -1, :] # Becomes ( B , C)\n",
    "            # apply softmax to get probablities\n",
    "            probs = F.softmax( logits, dim=-1) # (B, C)\n",
    "            # Sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # ( B, 1)\n",
    "            # appened sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, t + 1)\n",
    "        return index\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd591c3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mGPTLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m m = model.to(device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mGPTLanguageModel.__init__\u001b[39m\u001b[34m(self, vocab_size)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size) : \n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_embedding_table\u001b[49m = nn.Embedding(vocab_size, n_embd)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.position_embedding_table = nn.Embedding(block_size, n_embd)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mself\u001b[39m.blocks = nn.Sequencial(* [Block(n_embd, n_head=n_head) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layer)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jeeva\\.conda\\envs\\LLM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1965\u001b[39m, in \u001b[36mModule.__setattr__\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   1963\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Module):\n\u001b[32m   1964\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1965\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1966\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcannot assign module before Module.__init__() call\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1967\u001b[39m         )\n\u001b[32m   1968\u001b[39m     remove_from(\n\u001b[32m   1969\u001b[39m         \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m,\n\u001b[32m   1970\u001b[39m         \u001b[38;5;28mself\u001b[39m._parameters,\n\u001b[32m   1971\u001b[39m         \u001b[38;5;28mself\u001b[39m._buffers,\n\u001b[32m   1972\u001b[39m         \u001b[38;5;28mself\u001b[39m._non_persistent_buffers_set,\n\u001b[32m   1973\u001b[39m     )\n\u001b[32m   1974\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m _global_module_registration_hooks.values():\n",
      "\u001b[31mAttributeError\u001b[39m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efcf729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention\"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C) -> out: (B, T, head_size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)    # (B, T, hs)\n",
    "        q = self.query(x)  # (B, T, hs)\n",
    "        # attention scores\n",
    "        wei = (q @ k.transpose(-2, -1)) * (k.shape[-1] ** -0.5)  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        # weighted sum of values\n",
    "        v = self.value(x)  # (B, T, hs)\n",
    "        out = wei @ v      # (B, T, hs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # (B, T, nh*hs)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()  # FIX: must call Module.__init__ first\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        tok_emb = self.token_embedding_table(index)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=index.device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)     # (B, T, C)\n",
    "        x = self.ln_f(x)       # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            b, t, c = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(b * t, c), targets.view(b * t))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index: (B, T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = index[:, -block_size:]\n",
    "            logits, _ = self.forward(idx_cond)\n",
    "            logits_last = logits[:, -1, :]            # (B, C)\n",
    "            probs = F.softmax(logits_last, dim=-1)    # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            index = torch.cat((index, idx_next), dim=1)         # (B, T+1)\n",
    "        return index\n",
    "\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "m = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "209c19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss() : \n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val'] : \n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters) :\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c848e377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 4.410, val loss: 4.411\n",
      "step: 1, loss: 4.398, val loss: 4.399\n",
      "step: 2, loss: 4.386, val loss: 4.388\n",
      "step: 3, loss: 4.381, val loss: 4.378\n",
      "step: 4, loss: 4.368, val loss: 4.370\n",
      "step: 5, loss: 4.357, val loss: 4.356\n",
      "step: 6, loss: 4.347, val loss: 4.347\n",
      "step: 7, loss: 4.337, val loss: 4.337\n",
      "step: 8, loss: 4.322, val loss: 4.331\n",
      "step: 9, loss: 4.319, val loss: 4.321\n",
      "step: 10, loss: 4.308, val loss: 4.312\n",
      "step: 11, loss: 4.299, val loss: 4.298\n",
      "step: 12, loss: 4.288, val loss: 4.289\n",
      "step: 13, loss: 4.278, val loss: 4.281\n",
      "step: 14, loss: 4.267, val loss: 4.267\n",
      "step: 15, loss: 4.262, val loss: 4.254\n",
      "step: 16, loss: 4.252, val loss: 4.241\n",
      "step: 17, loss: 4.238, val loss: 4.232\n",
      "step: 18, loss: 4.226, val loss: 4.226\n",
      "step: 19, loss: 4.213, val loss: 4.216\n",
      "step: 20, loss: 4.196, val loss: 4.199\n",
      "step: 21, loss: 4.186, val loss: 4.190\n",
      "step: 22, loss: 4.174, val loss: 4.178\n",
      "step: 23, loss: 4.165, val loss: 4.165\n",
      "step: 24, loss: 4.159, val loss: 4.151\n",
      "step: 25, loss: 4.142, val loss: 4.141\n",
      "step: 26, loss: 4.129, val loss: 4.129\n",
      "step: 27, loss: 4.112, val loss: 4.112\n",
      "step: 28, loss: 4.104, val loss: 4.103\n",
      "step: 29, loss: 4.088, val loss: 4.091\n",
      "step: 30, loss: 4.079, val loss: 4.077\n",
      "step: 31, loss: 4.072, val loss: 4.066\n",
      "step: 32, loss: 4.055, val loss: 4.064\n",
      "step: 33, loss: 4.041, val loss: 4.041\n",
      "step: 34, loss: 4.021, val loss: 4.024\n",
      "step: 35, loss: 4.015, val loss: 4.015\n",
      "step: 36, loss: 4.000, val loss: 3.994\n",
      "step: 37, loss: 3.980, val loss: 3.981\n",
      "step: 38, loss: 3.962, val loss: 3.963\n",
      "step: 39, loss: 3.957, val loss: 3.955\n",
      "step: 40, loss: 3.952, val loss: 3.933\n",
      "step: 41, loss: 3.914, val loss: 3.909\n",
      "step: 42, loss: 3.911, val loss: 3.904\n",
      "step: 43, loss: 3.881, val loss: 3.884\n",
      "step: 44, loss: 3.870, val loss: 3.878\n",
      "step: 45, loss: 3.848, val loss: 3.861\n",
      "step: 46, loss: 3.838, val loss: 3.837\n",
      "step: 47, loss: 3.823, val loss: 3.835\n",
      "step: 48, loss: 3.808, val loss: 3.801\n",
      "step: 49, loss: 3.781, val loss: 3.792\n",
      "step: 50, loss: 3.762, val loss: 3.783\n",
      "step: 51, loss: 3.757, val loss: 3.753\n",
      "step: 52, loss: 3.741, val loss: 3.735\n",
      "step: 53, loss: 3.725, val loss: 3.724\n",
      "step: 54, loss: 3.719, val loss: 3.710\n",
      "step: 55, loss: 3.697, val loss: 3.683\n",
      "step: 56, loss: 3.673, val loss: 3.670\n",
      "step: 57, loss: 3.661, val loss: 3.653\n",
      "step: 58, loss: 3.643, val loss: 3.635\n",
      "step: 59, loss: 3.636, val loss: 3.644\n",
      "step: 60, loss: 3.609, val loss: 3.603\n",
      "step: 61, loss: 3.599, val loss: 3.589\n",
      "step: 62, loss: 3.585, val loss: 3.587\n",
      "step: 63, loss: 3.568, val loss: 3.572\n",
      "step: 64, loss: 3.554, val loss: 3.551\n",
      "step: 65, loss: 3.554, val loss: 3.545\n",
      "step: 66, loss: 3.523, val loss: 3.540\n",
      "step: 67, loss: 3.509, val loss: 3.514\n",
      "step: 68, loss: 3.511, val loss: 3.505\n",
      "step: 69, loss: 3.488, val loss: 3.501\n",
      "step: 70, loss: 3.483, val loss: 3.476\n",
      "step: 71, loss: 3.469, val loss: 3.465\n",
      "step: 72, loss: 3.476, val loss: 3.459\n",
      "step: 73, loss: 3.456, val loss: 3.447\n",
      "step: 74, loss: 3.429, val loss: 3.429\n",
      "step: 75, loss: 3.429, val loss: 3.430\n",
      "step: 76, loss: 3.421, val loss: 3.404\n",
      "step: 77, loss: 3.431, val loss: 3.416\n",
      "step: 78, loss: 3.396, val loss: 3.395\n",
      "step: 79, loss: 3.386, val loss: 3.394\n",
      "step: 80, loss: 3.392, val loss: 3.369\n",
      "step: 81, loss: 3.363, val loss: 3.369\n",
      "step: 82, loss: 3.370, val loss: 3.384\n",
      "step: 83, loss: 3.358, val loss: 3.368\n",
      "step: 84, loss: 3.381, val loss: 3.346\n",
      "step: 85, loss: 3.353, val loss: 3.344\n",
      "step: 86, loss: 3.331, val loss: 3.349\n",
      "step: 87, loss: 3.342, val loss: 3.337\n",
      "step: 88, loss: 3.339, val loss: 3.314\n",
      "step: 89, loss: 3.327, val loss: 3.330\n",
      "step: 90, loss: 3.312, val loss: 3.323\n",
      "step: 91, loss: 3.302, val loss: 3.298\n",
      "step: 92, loss: 3.313, val loss: 3.310\n",
      "step: 93, loss: 3.299, val loss: 3.302\n",
      "step: 94, loss: 3.298, val loss: 3.304\n",
      "step: 95, loss: 3.299, val loss: 3.297\n",
      "step: 96, loss: 3.284, val loss: 3.265\n",
      "step: 97, loss: 3.265, val loss: 3.290\n",
      "step: 98, loss: 3.275, val loss: 3.265\n",
      "step: 99, loss: 3.259, val loss: 3.291\n",
      "step: 100, loss: 3.248, val loss: 3.276\n",
      "step: 101, loss: 3.244, val loss: 3.249\n",
      "step: 102, loss: 3.256, val loss: 3.272\n",
      "step: 103, loss: 3.254, val loss: 3.253\n",
      "step: 104, loss: 3.235, val loss: 3.256\n",
      "step: 105, loss: 3.257, val loss: 3.252\n",
      "step: 106, loss: 3.276, val loss: 3.237\n",
      "step: 107, loss: 3.231, val loss: 3.235\n",
      "step: 108, loss: 3.230, val loss: 3.245\n",
      "step: 109, loss: 3.236, val loss: 3.227\n",
      "step: 110, loss: 3.210, val loss: 3.260\n",
      "step: 111, loss: 3.232, val loss: 3.245\n",
      "step: 112, loss: 3.225, val loss: 3.195\n",
      "step: 113, loss: 3.204, val loss: 3.227\n",
      "step: 114, loss: 3.210, val loss: 3.213\n",
      "step: 115, loss: 3.197, val loss: 3.204\n",
      "step: 116, loss: 3.217, val loss: 3.234\n",
      "step: 117, loss: 3.196, val loss: 3.202\n",
      "step: 118, loss: 3.175, val loss: 3.194\n",
      "step: 119, loss: 3.192, val loss: 3.193\n",
      "step: 120, loss: 3.203, val loss: 3.203\n",
      "step: 121, loss: 3.179, val loss: 3.210\n",
      "step: 122, loss: 3.165, val loss: 3.190\n",
      "step: 123, loss: 3.172, val loss: 3.183\n",
      "step: 124, loss: 3.207, val loss: 3.201\n",
      "step: 125, loss: 3.174, val loss: 3.189\n",
      "step: 126, loss: 3.178, val loss: 3.189\n",
      "step: 127, loss: 3.169, val loss: 3.155\n",
      "step: 128, loss: 3.173, val loss: 3.169\n",
      "step: 129, loss: 3.160, val loss: 3.151\n",
      "step: 130, loss: 3.176, val loss: 3.157\n",
      "step: 131, loss: 3.149, val loss: 3.166\n",
      "step: 132, loss: 3.169, val loss: 3.185\n",
      "step: 133, loss: 3.174, val loss: 3.166\n",
      "step: 134, loss: 3.154, val loss: 3.141\n",
      "step: 135, loss: 3.137, val loss: 3.155\n",
      "step: 136, loss: 3.136, val loss: 3.156\n",
      "step: 137, loss: 3.153, val loss: 3.158\n",
      "step: 138, loss: 3.165, val loss: 3.151\n",
      "step: 139, loss: 3.162, val loss: 3.123\n",
      "step: 140, loss: 3.126, val loss: 3.118\n",
      "step: 141, loss: 3.125, val loss: 3.154\n",
      "step: 142, loss: 3.136, val loss: 3.131\n",
      "step: 143, loss: 3.128, val loss: 3.133\n",
      "step: 144, loss: 3.131, val loss: 3.118\n",
      "step: 145, loss: 3.142, val loss: 3.116\n",
      "step: 146, loss: 3.138, val loss: 3.112\n",
      "step: 147, loss: 3.138, val loss: 3.134\n",
      "step: 148, loss: 3.123, val loss: 3.120\n",
      "step: 149, loss: 3.112, val loss: 3.085\n",
      "step: 150, loss: 3.120, val loss: 3.094\n",
      "step: 151, loss: 3.119, val loss: 3.095\n",
      "step: 152, loss: 3.128, val loss: 3.099\n",
      "step: 153, loss: 3.104, val loss: 3.114\n",
      "step: 154, loss: 3.096, val loss: 3.092\n",
      "step: 155, loss: 3.088, val loss: 3.105\n",
      "step: 156, loss: 3.121, val loss: 3.111\n",
      "step: 157, loss: 3.083, val loss: 3.083\n",
      "step: 158, loss: 3.080, val loss: 3.079\n",
      "step: 159, loss: 3.089, val loss: 3.087\n",
      "step: 160, loss: 3.070, val loss: 3.091\n",
      "step: 161, loss: 3.133, val loss: 3.130\n",
      "step: 162, loss: 3.080, val loss: 3.057\n",
      "step: 163, loss: 3.072, val loss: 3.095\n",
      "step: 164, loss: 3.089, val loss: 3.072\n",
      "step: 165, loss: 3.058, val loss: 3.087\n",
      "step: 166, loss: 3.087, val loss: 3.058\n",
      "step: 167, loss: 3.067, val loss: 3.057\n",
      "step: 168, loss: 3.086, val loss: 3.092\n",
      "step: 169, loss: 3.071, val loss: 3.082\n",
      "step: 170, loss: 3.069, val loss: 3.078\n",
      "step: 171, loss: 3.047, val loss: 3.083\n",
      "step: 172, loss: 3.054, val loss: 3.071\n",
      "step: 173, loss: 3.081, val loss: 3.074\n",
      "step: 174, loss: 3.043, val loss: 3.064\n",
      "step: 175, loss: 3.052, val loss: 3.077\n",
      "step: 176, loss: 3.038, val loss: 3.064\n",
      "step: 177, loss: 3.081, val loss: 3.077\n",
      "step: 178, loss: 3.060, val loss: 3.065\n",
      "step: 179, loss: 3.054, val loss: 3.053\n",
      "step: 180, loss: 3.026, val loss: 2.996\n",
      "step: 181, loss: 3.037, val loss: 3.076\n",
      "step: 182, loss: 3.036, val loss: 3.020\n",
      "step: 183, loss: 3.046, val loss: 3.035\n",
      "step: 184, loss: 3.025, val loss: 3.038\n",
      "step: 185, loss: 3.028, val loss: 3.038\n",
      "step: 186, loss: 3.029, val loss: 3.013\n",
      "step: 187, loss: 3.015, val loss: 3.032\n",
      "step: 188, loss: 3.049, val loss: 3.049\n",
      "step: 189, loss: 3.018, val loss: 3.043\n",
      "step: 190, loss: 3.016, val loss: 3.038\n",
      "step: 191, loss: 3.002, val loss: 3.029\n",
      "step: 192, loss: 3.024, val loss: 3.040\n",
      "step: 193, loss: 3.043, val loss: 3.037\n",
      "step: 194, loss: 3.042, val loss: 3.037\n",
      "step: 195, loss: 3.001, val loss: 3.005\n",
      "step: 196, loss: 3.032, val loss: 3.018\n",
      "step: 197, loss: 3.002, val loss: 2.996\n",
      "step: 198, loss: 3.005, val loss: 3.021\n",
      "step: 199, loss: 2.993, val loss: 3.027\n",
      "step: 200, loss: 3.010, val loss: 3.013\n",
      "step: 201, loss: 2.984, val loss: 2.986\n",
      "step: 202, loss: 2.979, val loss: 3.036\n",
      "step: 203, loss: 3.026, val loss: 3.021\n",
      "step: 204, loss: 2.999, val loss: 3.008\n",
      "step: 205, loss: 2.992, val loss: 3.023\n",
      "step: 206, loss: 3.001, val loss: 3.026\n",
      "step: 207, loss: 3.006, val loss: 2.964\n",
      "step: 208, loss: 2.984, val loss: 3.027\n",
      "step: 209, loss: 2.971, val loss: 3.008\n",
      "step: 210, loss: 2.997, val loss: 3.001\n",
      "step: 211, loss: 2.977, val loss: 2.990\n",
      "step: 212, loss: 2.977, val loss: 3.023\n",
      "step: 213, loss: 2.944, val loss: 2.966\n",
      "step: 214, loss: 2.962, val loss: 3.008\n",
      "step: 215, loss: 2.978, val loss: 2.971\n",
      "step: 216, loss: 2.975, val loss: 2.986\n",
      "step: 217, loss: 2.948, val loss: 2.973\n",
      "step: 218, loss: 2.981, val loss: 2.956\n",
      "step: 219, loss: 2.980, val loss: 2.948\n",
      "step: 220, loss: 2.969, val loss: 2.937\n",
      "step: 221, loss: 2.963, val loss: 2.963\n",
      "step: 222, loss: 2.985, val loss: 2.968\n",
      "step: 223, loss: 2.941, val loss: 2.991\n",
      "step: 224, loss: 2.957, val loss: 2.965\n",
      "step: 225, loss: 2.927, val loss: 2.960\n",
      "step: 226, loss: 2.952, val loss: 2.950\n",
      "step: 227, loss: 2.912, val loss: 2.951\n",
      "step: 228, loss: 2.929, val loss: 2.969\n",
      "step: 229, loss: 2.935, val loss: 2.950\n",
      "step: 230, loss: 2.957, val loss: 2.906\n",
      "step: 231, loss: 2.927, val loss: 2.952\n",
      "step: 232, loss: 2.932, val loss: 2.923\n",
      "step: 233, loss: 2.950, val loss: 2.930\n",
      "step: 234, loss: 2.927, val loss: 2.933\n",
      "step: 235, loss: 2.917, val loss: 2.906\n",
      "step: 236, loss: 2.912, val loss: 2.920\n",
      "step: 237, loss: 2.908, val loss: 2.985\n",
      "step: 238, loss: 2.893, val loss: 2.924\n",
      "step: 239, loss: 2.929, val loss: 2.901\n",
      "step: 240, loss: 2.932, val loss: 2.922\n",
      "step: 241, loss: 2.938, val loss: 2.898\n",
      "step: 242, loss: 2.958, val loss: 2.893\n",
      "step: 243, loss: 2.927, val loss: 2.914\n",
      "step: 244, loss: 2.907, val loss: 2.912\n",
      "step: 245, loss: 2.906, val loss: 2.928\n",
      "step: 246, loss: 2.900, val loss: 2.937\n",
      "step: 247, loss: 2.891, val loss: 2.905\n",
      "step: 248, loss: 2.869, val loss: 2.894\n",
      "step: 249, loss: 2.899, val loss: 2.912\n",
      "step: 250, loss: 2.883, val loss: 2.897\n",
      "step: 251, loss: 2.892, val loss: 2.912\n",
      "step: 252, loss: 2.888, val loss: 2.895\n",
      "step: 253, loss: 2.905, val loss: 2.905\n",
      "step: 254, loss: 2.878, val loss: 2.888\n",
      "step: 255, loss: 2.882, val loss: 2.898\n",
      "step: 256, loss: 2.913, val loss: 2.866\n",
      "step: 257, loss: 2.887, val loss: 2.873\n",
      "step: 258, loss: 2.897, val loss: 2.909\n",
      "step: 259, loss: 2.916, val loss: 2.863\n",
      "step: 260, loss: 2.879, val loss: 2.902\n",
      "step: 261, loss: 2.874, val loss: 2.846\n",
      "step: 262, loss: 2.888, val loss: 2.863\n",
      "step: 263, loss: 2.871, val loss: 2.878\n",
      "step: 264, loss: 2.895, val loss: 2.872\n",
      "step: 265, loss: 2.849, val loss: 2.848\n",
      "step: 266, loss: 2.873, val loss: 2.832\n",
      "step: 267, loss: 2.868, val loss: 2.867\n",
      "step: 268, loss: 2.868, val loss: 2.870\n",
      "step: 269, loss: 2.862, val loss: 2.850\n",
      "step: 270, loss: 2.864, val loss: 2.890\n",
      "step: 271, loss: 2.846, val loss: 2.878\n",
      "step: 272, loss: 2.844, val loss: 2.876\n",
      "step: 273, loss: 2.855, val loss: 2.847\n",
      "step: 274, loss: 2.832, val loss: 2.827\n",
      "step: 275, loss: 2.829, val loss: 2.858\n",
      "step: 276, loss: 2.869, val loss: 2.861\n",
      "step: 277, loss: 2.865, val loss: 2.868\n",
      "step: 278, loss: 2.836, val loss: 2.854\n",
      "step: 279, loss: 2.887, val loss: 2.864\n",
      "step: 280, loss: 2.854, val loss: 2.842\n",
      "step: 281, loss: 2.860, val loss: 2.823\n",
      "step: 282, loss: 2.800, val loss: 2.837\n",
      "step: 283, loss: 2.839, val loss: 2.839\n",
      "step: 284, loss: 2.822, val loss: 2.847\n",
      "step: 285, loss: 2.835, val loss: 2.851\n",
      "step: 286, loss: 2.827, val loss: 2.821\n",
      "step: 287, loss: 2.838, val loss: 2.853\n",
      "step: 288, loss: 2.859, val loss: 2.816\n",
      "step: 289, loss: 2.819, val loss: 2.809\n",
      "step: 290, loss: 2.817, val loss: 2.839\n",
      "step: 291, loss: 2.813, val loss: 2.807\n",
      "step: 292, loss: 2.799, val loss: 2.792\n",
      "step: 293, loss: 2.836, val loss: 2.805\n",
      "step: 294, loss: 2.843, val loss: 2.842\n",
      "step: 295, loss: 2.835, val loss: 2.834\n",
      "step: 296, loss: 2.811, val loss: 2.815\n",
      "step: 297, loss: 2.801, val loss: 2.794\n",
      "step: 298, loss: 2.816, val loss: 2.810\n",
      "step: 299, loss: 2.780, val loss: 2.822\n",
      "step: 300, loss: 2.802, val loss: 2.784\n",
      "step: 301, loss: 2.837, val loss: 2.815\n",
      "step: 302, loss: 2.804, val loss: 2.783\n",
      "step: 303, loss: 2.840, val loss: 2.819\n",
      "step: 304, loss: 2.822, val loss: 2.848\n",
      "step: 305, loss: 2.774, val loss: 2.824\n",
      "step: 306, loss: 2.810, val loss: 2.766\n",
      "step: 307, loss: 2.794, val loss: 2.798\n",
      "step: 308, loss: 2.781, val loss: 2.834\n",
      "step: 309, loss: 2.804, val loss: 2.812\n",
      "step: 310, loss: 2.820, val loss: 2.789\n",
      "step: 311, loss: 2.743, val loss: 2.784\n",
      "step: 312, loss: 2.787, val loss: 2.787\n",
      "step: 313, loss: 2.789, val loss: 2.799\n",
      "step: 314, loss: 2.773, val loss: 2.799\n",
      "step: 315, loss: 2.784, val loss: 2.789\n",
      "step: 316, loss: 2.783, val loss: 2.762\n",
      "step: 317, loss: 2.784, val loss: 2.791\n",
      "step: 318, loss: 2.809, val loss: 2.804\n",
      "step: 319, loss: 2.779, val loss: 2.769\n",
      "step: 320, loss: 2.780, val loss: 2.764\n",
      "step: 321, loss: 2.793, val loss: 2.772\n",
      "step: 322, loss: 2.773, val loss: 2.764\n",
      "step: 323, loss: 2.756, val loss: 2.773\n",
      "step: 324, loss: 2.770, val loss: 2.755\n",
      "step: 325, loss: 2.769, val loss: 2.776\n",
      "step: 326, loss: 2.785, val loss: 2.780\n",
      "step: 327, loss: 2.770, val loss: 2.808\n",
      "step: 328, loss: 2.819, val loss: 2.777\n",
      "step: 329, loss: 2.824, val loss: 2.765\n",
      "step: 330, loss: 2.783, val loss: 2.747\n",
      "step: 331, loss: 2.745, val loss: 2.776\n",
      "step: 332, loss: 2.727, val loss: 2.831\n",
      "step: 333, loss: 2.768, val loss: 2.776\n",
      "step: 334, loss: 2.766, val loss: 2.783\n",
      "step: 335, loss: 2.767, val loss: 2.799\n",
      "step: 336, loss: 2.762, val loss: 2.748\n",
      "step: 337, loss: 2.754, val loss: 2.755\n",
      "step: 338, loss: 2.801, val loss: 2.768\n",
      "step: 339, loss: 2.731, val loss: 2.753\n",
      "step: 340, loss: 2.750, val loss: 2.731\n",
      "step: 341, loss: 2.746, val loss: 2.749\n",
      "step: 342, loss: 2.783, val loss: 2.756\n",
      "step: 343, loss: 2.770, val loss: 2.768\n",
      "step: 344, loss: 2.755, val loss: 2.770\n",
      "step: 345, loss: 2.754, val loss: 2.753\n",
      "step: 346, loss: 2.742, val loss: 2.774\n",
      "step: 347, loss: 2.744, val loss: 2.799\n",
      "step: 348, loss: 2.729, val loss: 2.753\n",
      "step: 349, loss: 2.738, val loss: 2.764\n",
      "step: 350, loss: 2.765, val loss: 2.752\n",
      "step: 351, loss: 2.740, val loss: 2.736\n",
      "step: 352, loss: 2.750, val loss: 2.742\n",
      "step: 353, loss: 2.735, val loss: 2.738\n",
      "step: 354, loss: 2.770, val loss: 2.739\n",
      "step: 355, loss: 2.720, val loss: 2.726\n",
      "step: 356, loss: 2.738, val loss: 2.767\n",
      "step: 357, loss: 2.712, val loss: 2.729\n",
      "step: 358, loss: 2.746, val loss: 2.734\n",
      "step: 359, loss: 2.739, val loss: 2.731\n",
      "step: 360, loss: 2.729, val loss: 2.726\n",
      "step: 361, loss: 2.762, val loss: 2.714\n",
      "step: 362, loss: 2.716, val loss: 2.748\n",
      "step: 363, loss: 2.752, val loss: 2.758\n",
      "step: 364, loss: 2.706, val loss: 2.733\n",
      "step: 365, loss: 2.739, val loss: 2.700\n",
      "step: 366, loss: 2.749, val loss: 2.755\n",
      "step: 367, loss: 2.748, val loss: 2.713\n",
      "step: 368, loss: 2.746, val loss: 2.736\n",
      "step: 369, loss: 2.750, val loss: 2.743\n",
      "step: 370, loss: 2.745, val loss: 2.706\n",
      "step: 371, loss: 2.722, val loss: 2.733\n",
      "step: 372, loss: 2.745, val loss: 2.710\n",
      "step: 373, loss: 2.717, val loss: 2.709\n",
      "step: 374, loss: 2.722, val loss: 2.703\n",
      "step: 375, loss: 2.716, val loss: 2.717\n",
      "step: 376, loss: 2.757, val loss: 2.723\n",
      "step: 377, loss: 2.688, val loss: 2.739\n",
      "step: 378, loss: 2.720, val loss: 2.746\n",
      "step: 379, loss: 2.720, val loss: 2.705\n",
      "step: 380, loss: 2.721, val loss: 2.699\n",
      "step: 381, loss: 2.721, val loss: 2.740\n",
      "step: 382, loss: 2.747, val loss: 2.689\n",
      "step: 383, loss: 2.741, val loss: 2.739\n",
      "step: 384, loss: 2.768, val loss: 2.703\n",
      "step: 385, loss: 2.747, val loss: 2.725\n",
      "step: 386, loss: 2.728, val loss: 2.748\n",
      "step: 387, loss: 2.722, val loss: 2.785\n",
      "step: 388, loss: 2.704, val loss: 2.696\n",
      "step: 389, loss: 2.782, val loss: 2.727\n",
      "step: 390, loss: 2.705, val loss: 2.684\n",
      "step: 391, loss: 2.721, val loss: 2.703\n",
      "step: 392, loss: 2.726, val loss: 2.702\n",
      "step: 393, loss: 2.716, val loss: 2.706\n",
      "step: 394, loss: 2.763, val loss: 2.718\n",
      "step: 395, loss: 2.719, val loss: 2.703\n",
      "step: 396, loss: 2.700, val loss: 2.744\n",
      "step: 397, loss: 2.719, val loss: 2.712\n",
      "step: 398, loss: 2.735, val loss: 2.724\n",
      "step: 399, loss: 2.682, val loss: 2.710\n",
      "step: 400, loss: 2.723, val loss: 2.708\n",
      "step: 401, loss: 2.699, val loss: 2.728\n",
      "step: 402, loss: 2.694, val loss: 2.713\n",
      "step: 403, loss: 2.700, val loss: 2.700\n",
      "step: 404, loss: 2.726, val loss: 2.707\n",
      "step: 405, loss: 2.682, val loss: 2.698\n",
      "step: 406, loss: 2.696, val loss: 2.710\n",
      "step: 407, loss: 2.717, val loss: 2.677\n",
      "step: 408, loss: 2.712, val loss: 2.708\n",
      "step: 409, loss: 2.695, val loss: 2.697\n",
      "step: 410, loss: 2.690, val loss: 2.676\n",
      "step: 411, loss: 2.721, val loss: 2.685\n",
      "step: 412, loss: 2.695, val loss: 2.685\n",
      "step: 413, loss: 2.707, val loss: 2.720\n",
      "step: 414, loss: 2.696, val loss: 2.706\n",
      "step: 415, loss: 2.666, val loss: 2.698\n",
      "step: 416, loss: 2.696, val loss: 2.714\n",
      "step: 417, loss: 2.725, val loss: 2.685\n",
      "step: 418, loss: 2.684, val loss: 2.700\n",
      "step: 419, loss: 2.708, val loss: 2.662\n",
      "step: 420, loss: 2.685, val loss: 2.732\n",
      "step: 421, loss: 2.653, val loss: 2.718\n",
      "step: 422, loss: 2.705, val loss: 2.694\n",
      "step: 423, loss: 2.738, val loss: 2.736\n",
      "step: 424, loss: 2.731, val loss: 2.718\n",
      "step: 425, loss: 2.709, val loss: 2.666\n",
      "step: 426, loss: 2.699, val loss: 2.634\n",
      "step: 427, loss: 2.696, val loss: 2.715\n",
      "step: 428, loss: 2.668, val loss: 2.686\n",
      "step: 429, loss: 2.707, val loss: 2.744\n",
      "step: 430, loss: 2.711, val loss: 2.705\n",
      "step: 431, loss: 2.673, val loss: 2.653\n",
      "step: 432, loss: 2.727, val loss: 2.681\n",
      "step: 433, loss: 2.668, val loss: 2.689\n",
      "step: 434, loss: 2.662, val loss: 2.655\n",
      "step: 435, loss: 2.716, val loss: 2.690\n",
      "step: 436, loss: 2.676, val loss: 2.669\n",
      "step: 437, loss: 2.697, val loss: 2.676\n",
      "step: 438, loss: 2.657, val loss: 2.667\n",
      "step: 439, loss: 2.676, val loss: 2.679\n",
      "step: 440, loss: 2.671, val loss: 2.703\n",
      "step: 441, loss: 2.726, val loss: 2.655\n",
      "step: 442, loss: 2.680, val loss: 2.657\n",
      "step: 443, loss: 2.670, val loss: 2.694\n",
      "step: 444, loss: 2.649, val loss: 2.660\n",
      "step: 445, loss: 2.681, val loss: 2.635\n",
      "step: 446, loss: 2.684, val loss: 2.678\n",
      "step: 447, loss: 2.704, val loss: 2.691\n",
      "step: 448, loss: 2.715, val loss: 2.687\n",
      "step: 449, loss: 2.665, val loss: 2.681\n",
      "step: 450, loss: 2.658, val loss: 2.675\n",
      "step: 451, loss: 2.703, val loss: 2.667\n",
      "step: 452, loss: 2.662, val loss: 2.688\n",
      "step: 453, loss: 2.628, val loss: 2.686\n",
      "step: 454, loss: 2.707, val loss: 2.664\n",
      "step: 455, loss: 2.670, val loss: 2.682\n",
      "step: 456, loss: 2.661, val loss: 2.689\n",
      "step: 457, loss: 2.667, val loss: 2.672\n",
      "step: 458, loss: 2.660, val loss: 2.661\n",
      "step: 459, loss: 2.662, val loss: 2.644\n",
      "step: 460, loss: 2.686, val loss: 2.638\n",
      "step: 461, loss: 2.737, val loss: 2.632\n",
      "step: 462, loss: 2.692, val loss: 2.683\n",
      "step: 463, loss: 2.622, val loss: 2.636\n",
      "step: 464, loss: 2.650, val loss: 2.685\n",
      "step: 465, loss: 2.627, val loss: 2.667\n",
      "step: 466, loss: 2.676, val loss: 2.694\n",
      "step: 467, loss: 2.664, val loss: 2.640\n",
      "step: 468, loss: 2.661, val loss: 2.677\n",
      "step: 469, loss: 2.698, val loss: 2.668\n",
      "step: 470, loss: 2.633, val loss: 2.625\n",
      "step: 471, loss: 2.639, val loss: 2.643\n",
      "step: 472, loss: 2.653, val loss: 2.662\n",
      "step: 473, loss: 2.639, val loss: 2.662\n",
      "step: 474, loss: 2.644, val loss: 2.655\n",
      "step: 475, loss: 2.646, val loss: 2.632\n",
      "step: 476, loss: 2.630, val loss: 2.641\n",
      "step: 477, loss: 2.658, val loss: 2.622\n",
      "step: 478, loss: 2.648, val loss: 2.688\n",
      "step: 479, loss: 2.657, val loss: 2.683\n",
      "step: 480, loss: 2.696, val loss: 2.650\n",
      "step: 481, loss: 2.678, val loss: 2.659\n",
      "step: 482, loss: 2.621, val loss: 2.675\n",
      "step: 483, loss: 2.689, val loss: 2.662\n",
      "step: 484, loss: 2.639, val loss: 2.665\n",
      "step: 485, loss: 2.654, val loss: 2.676\n",
      "step: 486, loss: 2.631, val loss: 2.612\n",
      "step: 487, loss: 2.661, val loss: 2.642\n",
      "step: 488, loss: 2.663, val loss: 2.629\n",
      "step: 489, loss: 2.641, val loss: 2.635\n",
      "step: 490, loss: 2.635, val loss: 2.670\n",
      "step: 491, loss: 2.647, val loss: 2.646\n",
      "step: 492, loss: 2.617, val loss: 2.632\n",
      "step: 493, loss: 2.655, val loss: 2.655\n",
      "step: 494, loss: 2.659, val loss: 2.651\n",
      "step: 495, loss: 2.667, val loss: 2.684\n",
      "step: 496, loss: 2.656, val loss: 2.654\n",
      "step: 497, loss: 2.648, val loss: 2.688\n",
      "step: 498, loss: 2.674, val loss: 2.663\n",
      "step: 499, loss: 2.613, val loss: 2.643\n",
      "step: 500, loss: 2.628, val loss: 2.638\n",
      "step: 501, loss: 2.649, val loss: 2.667\n",
      "step: 502, loss: 2.631, val loss: 2.663\n",
      "step: 503, loss: 2.651, val loss: 2.628\n",
      "step: 504, loss: 2.646, val loss: 2.655\n",
      "step: 505, loss: 2.646, val loss: 2.656\n",
      "step: 506, loss: 2.672, val loss: 2.630\n",
      "step: 507, loss: 2.680, val loss: 2.650\n",
      "step: 508, loss: 2.650, val loss: 2.672\n",
      "step: 509, loss: 2.655, val loss: 2.641\n",
      "step: 510, loss: 2.627, val loss: 2.626\n",
      "step: 511, loss: 2.677, val loss: 2.628\n",
      "step: 512, loss: 2.642, val loss: 2.637\n",
      "step: 513, loss: 2.603, val loss: 2.623\n",
      "step: 514, loss: 2.632, val loss: 2.666\n",
      "step: 515, loss: 2.643, val loss: 2.645\n",
      "step: 516, loss: 2.654, val loss: 2.616\n",
      "step: 517, loss: 2.618, val loss: 2.619\n",
      "step: 518, loss: 2.611, val loss: 2.677\n",
      "step: 519, loss: 2.625, val loss: 2.638\n",
      "step: 520, loss: 2.624, val loss: 2.648\n",
      "step: 521, loss: 2.647, val loss: 2.604\n",
      "step: 522, loss: 2.611, val loss: 2.629\n",
      "step: 523, loss: 2.623, val loss: 2.605\n",
      "step: 524, loss: 2.644, val loss: 2.649\n",
      "step: 525, loss: 2.622, val loss: 2.611\n",
      "step: 526, loss: 2.672, val loss: 2.626\n",
      "step: 527, loss: 2.604, val loss: 2.632\n",
      "step: 528, loss: 2.590, val loss: 2.649\n",
      "step: 529, loss: 2.600, val loss: 2.667\n",
      "step: 530, loss: 2.599, val loss: 2.606\n",
      "step: 531, loss: 2.678, val loss: 2.620\n",
      "step: 532, loss: 2.661, val loss: 2.632\n",
      "step: 533, loss: 2.655, val loss: 2.623\n",
      "step: 534, loss: 2.617, val loss: 2.589\n",
      "step: 535, loss: 2.640, val loss: 2.638\n",
      "step: 536, loss: 2.635, val loss: 2.627\n",
      "step: 537, loss: 2.621, val loss: 2.642\n",
      "step: 538, loss: 2.632, val loss: 2.569\n",
      "step: 539, loss: 2.642, val loss: 2.686\n",
      "step: 540, loss: 2.690, val loss: 2.598\n",
      "step: 541, loss: 2.583, val loss: 2.667\n",
      "step: 542, loss: 2.633, val loss: 2.606\n",
      "step: 543, loss: 2.644, val loss: 2.620\n",
      "step: 544, loss: 2.643, val loss: 2.608\n",
      "step: 545, loss: 2.628, val loss: 2.604\n",
      "step: 546, loss: 2.625, val loss: 2.622\n",
      "step: 547, loss: 2.624, val loss: 2.648\n",
      "step: 548, loss: 2.602, val loss: 2.613\n",
      "step: 549, loss: 2.647, val loss: 2.616\n",
      "step: 550, loss: 2.618, val loss: 2.623\n",
      "step: 551, loss: 2.605, val loss: 2.654\n",
      "step: 552, loss: 2.637, val loss: 2.608\n",
      "step: 553, loss: 2.589, val loss: 2.641\n",
      "step: 554, loss: 2.583, val loss: 2.676\n",
      "step: 555, loss: 2.611, val loss: 2.650\n",
      "step: 556, loss: 2.673, val loss: 2.647\n",
      "step: 557, loss: 2.633, val loss: 2.635\n",
      "step: 558, loss: 2.631, val loss: 2.600\n",
      "step: 559, loss: 2.636, val loss: 2.646\n",
      "step: 560, loss: 2.602, val loss: 2.610\n",
      "step: 561, loss: 2.593, val loss: 2.586\n",
      "step: 562, loss: 2.627, val loss: 2.687\n",
      "step: 563, loss: 2.634, val loss: 2.645\n",
      "step: 564, loss: 2.589, val loss: 2.592\n",
      "step: 565, loss: 2.621, val loss: 2.647\n",
      "step: 566, loss: 2.616, val loss: 2.597\n",
      "step: 567, loss: 2.673, val loss: 2.616\n",
      "step: 568, loss: 2.596, val loss: 2.615\n",
      "step: 569, loss: 2.583, val loss: 2.599\n",
      "step: 570, loss: 2.625, val loss: 2.615\n",
      "step: 571, loss: 2.635, val loss: 2.624\n",
      "step: 572, loss: 2.600, val loss: 2.626\n",
      "step: 573, loss: 2.617, val loss: 2.632\n",
      "step: 574, loss: 2.650, val loss: 2.635\n",
      "step: 575, loss: 2.593, val loss: 2.626\n",
      "step: 576, loss: 2.634, val loss: 2.643\n",
      "step: 577, loss: 2.602, val loss: 2.649\n",
      "step: 578, loss: 2.636, val loss: 2.606\n",
      "step: 579, loss: 2.639, val loss: 2.612\n",
      "step: 580, loss: 2.665, val loss: 2.620\n",
      "step: 581, loss: 2.652, val loss: 2.610\n",
      "step: 582, loss: 2.658, val loss: 2.591\n",
      "step: 583, loss: 2.641, val loss: 2.596\n",
      "step: 584, loss: 2.610, val loss: 2.605\n",
      "step: 585, loss: 2.615, val loss: 2.609\n",
      "step: 586, loss: 2.597, val loss: 2.617\n",
      "step: 587, loss: 2.610, val loss: 2.642\n",
      "step: 588, loss: 2.642, val loss: 2.710\n",
      "step: 589, loss: 2.635, val loss: 2.611\n",
      "step: 590, loss: 2.628, val loss: 2.649\n",
      "step: 591, loss: 2.620, val loss: 2.576\n",
      "step: 592, loss: 2.630, val loss: 2.614\n",
      "step: 593, loss: 2.636, val loss: 2.606\n",
      "step: 594, loss: 2.641, val loss: 2.612\n",
      "step: 595, loss: 2.609, val loss: 2.603\n",
      "step: 596, loss: 2.614, val loss: 2.617\n",
      "step: 597, loss: 2.608, val loss: 2.635\n",
      "step: 598, loss: 2.629, val loss: 2.598\n",
      "step: 599, loss: 2.609, val loss: 2.650\n",
      "step: 600, loss: 2.662, val loss: 2.629\n",
      "step: 601, loss: 2.597, val loss: 2.616\n",
      "step: 602, loss: 2.605, val loss: 2.574\n",
      "step: 603, loss: 2.660, val loss: 2.621\n",
      "step: 604, loss: 2.591, val loss: 2.584\n",
      "step: 605, loss: 2.633, val loss: 2.576\n",
      "step: 606, loss: 2.589, val loss: 2.596\n",
      "step: 607, loss: 2.578, val loss: 2.573\n",
      "step: 608, loss: 2.581, val loss: 2.617\n",
      "step: 609, loss: 2.583, val loss: 2.580\n",
      "step: 610, loss: 2.594, val loss: 2.603\n",
      "step: 611, loss: 2.586, val loss: 2.610\n",
      "step: 612, loss: 2.647, val loss: 2.583\n",
      "step: 613, loss: 2.610, val loss: 2.587\n",
      "step: 614, loss: 2.607, val loss: 2.644\n",
      "step: 615, loss: 2.558, val loss: 2.597\n",
      "step: 616, loss: 2.608, val loss: 2.591\n",
      "step: 617, loss: 2.622, val loss: 2.586\n",
      "step: 618, loss: 2.594, val loss: 2.613\n",
      "step: 619, loss: 2.583, val loss: 2.652\n",
      "step: 620, loss: 2.619, val loss: 2.614\n",
      "step: 621, loss: 2.574, val loss: 2.577\n",
      "step: 622, loss: 2.583, val loss: 2.576\n",
      "step: 623, loss: 2.631, val loss: 2.616\n",
      "step: 624, loss: 2.615, val loss: 2.614\n",
      "step: 625, loss: 2.604, val loss: 2.571\n",
      "step: 626, loss: 2.609, val loss: 2.608\n",
      "step: 627, loss: 2.578, val loss: 2.602\n",
      "step: 628, loss: 2.592, val loss: 2.608\n",
      "step: 629, loss: 2.581, val loss: 2.630\n",
      "step: 630, loss: 2.623, val loss: 2.569\n",
      "step: 631, loss: 2.622, val loss: 2.647\n",
      "step: 632, loss: 2.569, val loss: 2.631\n",
      "step: 633, loss: 2.625, val loss: 2.614\n",
      "step: 634, loss: 2.625, val loss: 2.648\n",
      "step: 635, loss: 2.645, val loss: 2.613\n",
      "step: 636, loss: 2.577, val loss: 2.623\n",
      "step: 637, loss: 2.602, val loss: 2.651\n",
      "step: 638, loss: 2.611, val loss: 2.609\n",
      "step: 639, loss: 2.642, val loss: 2.549\n",
      "step: 640, loss: 2.598, val loss: 2.566\n",
      "step: 641, loss: 2.647, val loss: 2.597\n",
      "step: 642, loss: 2.592, val loss: 2.584\n",
      "step: 643, loss: 2.574, val loss: 2.604\n",
      "step: 644, loss: 2.575, val loss: 2.597\n",
      "step: 645, loss: 2.583, val loss: 2.642\n",
      "step: 646, loss: 2.573, val loss: 2.605\n",
      "step: 647, loss: 2.600, val loss: 2.618\n",
      "step: 648, loss: 2.610, val loss: 2.614\n",
      "step: 649, loss: 2.644, val loss: 2.544\n",
      "step: 650, loss: 2.582, val loss: 2.614\n",
      "step: 651, loss: 2.612, val loss: 2.547\n",
      "step: 652, loss: 2.605, val loss: 2.626\n",
      "step: 653, loss: 2.570, val loss: 2.572\n",
      "step: 654, loss: 2.605, val loss: 2.602\n",
      "step: 655, loss: 2.605, val loss: 2.600\n",
      "step: 656, loss: 2.573, val loss: 2.618\n",
      "step: 657, loss: 2.620, val loss: 2.591\n",
      "step: 658, loss: 2.619, val loss: 2.581\n",
      "step: 659, loss: 2.610, val loss: 2.577\n",
      "step: 660, loss: 2.617, val loss: 2.606\n",
      "step: 661, loss: 2.658, val loss: 2.607\n",
      "step: 662, loss: 2.661, val loss: 2.568\n",
      "step: 663, loss: 2.558, val loss: 2.580\n",
      "step: 664, loss: 2.573, val loss: 2.617\n",
      "step: 665, loss: 2.569, val loss: 2.602\n",
      "step: 666, loss: 2.563, val loss: 2.616\n",
      "step: 667, loss: 2.566, val loss: 2.600\n",
      "step: 668, loss: 2.589, val loss: 2.594\n",
      "step: 669, loss: 2.579, val loss: 2.682\n",
      "step: 670, loss: 2.586, val loss: 2.625\n",
      "step: 671, loss: 2.550, val loss: 2.579\n",
      "step: 672, loss: 2.570, val loss: 2.619\n",
      "step: 673, loss: 2.574, val loss: 2.608\n",
      "step: 674, loss: 2.585, val loss: 2.560\n",
      "step: 675, loss: 2.584, val loss: 2.611\n",
      "step: 676, loss: 2.581, val loss: 2.591\n",
      "step: 677, loss: 2.573, val loss: 2.591\n",
      "step: 678, loss: 2.645, val loss: 2.614\n",
      "step: 679, loss: 2.595, val loss: 2.581\n",
      "step: 680, loss: 2.565, val loss: 2.624\n",
      "step: 681, loss: 2.567, val loss: 2.618\n",
      "step: 682, loss: 2.609, val loss: 2.632\n",
      "step: 683, loss: 2.590, val loss: 2.556\n",
      "step: 684, loss: 2.553, val loss: 2.639\n",
      "step: 685, loss: 2.569, val loss: 2.614\n",
      "step: 686, loss: 2.597, val loss: 2.582\n",
      "step: 687, loss: 2.567, val loss: 2.538\n",
      "step: 688, loss: 2.654, val loss: 2.574\n",
      "step: 689, loss: 2.627, val loss: 2.602\n",
      "step: 690, loss: 2.576, val loss: 2.575\n",
      "step: 691, loss: 2.651, val loss: 2.585\n",
      "step: 692, loss: 2.618, val loss: 2.578\n",
      "step: 693, loss: 2.586, val loss: 2.581\n",
      "step: 694, loss: 2.582, val loss: 2.599\n",
      "step: 695, loss: 2.598, val loss: 2.564\n",
      "step: 696, loss: 2.579, val loss: 2.578\n",
      "step: 697, loss: 2.571, val loss: 2.646\n",
      "step: 698, loss: 2.596, val loss: 2.619\n",
      "step: 699, loss: 2.618, val loss: 2.563\n",
      "step: 700, loss: 2.653, val loss: 2.619\n",
      "step: 701, loss: 2.605, val loss: 2.553\n",
      "step: 702, loss: 2.608, val loss: 2.596\n",
      "step: 703, loss: 2.574, val loss: 2.579\n",
      "step: 704, loss: 2.616, val loss: 2.577\n",
      "step: 705, loss: 2.575, val loss: 2.575\n",
      "step: 706, loss: 2.603, val loss: 2.626\n",
      "step: 707, loss: 2.570, val loss: 2.595\n",
      "step: 708, loss: 2.608, val loss: 2.616\n",
      "step: 709, loss: 2.577, val loss: 2.569\n",
      "step: 710, loss: 2.578, val loss: 2.581\n",
      "step: 711, loss: 2.590, val loss: 2.592\n",
      "step: 712, loss: 2.543, val loss: 2.591\n",
      "step: 713, loss: 2.591, val loss: 2.622\n",
      "step: 714, loss: 2.618, val loss: 2.665\n",
      "step: 715, loss: 2.568, val loss: 2.587\n",
      "step: 716, loss: 2.602, val loss: 2.609\n",
      "step: 717, loss: 2.569, val loss: 2.582\n",
      "step: 718, loss: 2.589, val loss: 2.574\n",
      "step: 719, loss: 2.614, val loss: 2.557\n",
      "step: 720, loss: 2.566, val loss: 2.620\n",
      "step: 721, loss: 2.563, val loss: 2.614\n",
      "step: 722, loss: 2.631, val loss: 2.602\n",
      "step: 723, loss: 2.542, val loss: 2.596\n",
      "step: 724, loss: 2.592, val loss: 2.563\n",
      "step: 725, loss: 2.566, val loss: 2.563\n",
      "step: 726, loss: 2.604, val loss: 2.562\n",
      "step: 727, loss: 2.608, val loss: 2.586\n",
      "step: 728, loss: 2.587, val loss: 2.559\n",
      "step: 729, loss: 2.605, val loss: 2.572\n",
      "step: 730, loss: 2.579, val loss: 2.596\n",
      "step: 731, loss: 2.556, val loss: 2.578\n",
      "step: 732, loss: 2.605, val loss: 2.586\n",
      "step: 733, loss: 2.594, val loss: 2.590\n",
      "step: 734, loss: 2.597, val loss: 2.583\n",
      "step: 735, loss: 2.576, val loss: 2.575\n",
      "step: 736, loss: 2.599, val loss: 2.547\n",
      "step: 737, loss: 2.570, val loss: 2.554\n",
      "step: 738, loss: 2.564, val loss: 2.557\n",
      "step: 739, loss: 2.587, val loss: 2.563\n",
      "step: 740, loss: 2.582, val loss: 2.624\n",
      "step: 741, loss: 2.581, val loss: 2.549\n",
      "step: 742, loss: 2.597, val loss: 2.602\n",
      "step: 743, loss: 2.569, val loss: 2.605\n",
      "step: 744, loss: 2.591, val loss: 2.539\n",
      "step: 745, loss: 2.601, val loss: 2.590\n",
      "step: 746, loss: 2.557, val loss: 2.552\n",
      "step: 747, loss: 2.577, val loss: 2.555\n",
      "step: 748, loss: 2.596, val loss: 2.551\n",
      "step: 749, loss: 2.579, val loss: 2.597\n",
      "step: 750, loss: 2.578, val loss: 2.576\n",
      "step: 751, loss: 2.562, val loss: 2.540\n",
      "step: 752, loss: 2.592, val loss: 2.619\n",
      "step: 753, loss: 2.554, val loss: 2.571\n",
      "step: 754, loss: 2.587, val loss: 2.586\n",
      "step: 755, loss: 2.583, val loss: 2.531\n",
      "step: 756, loss: 2.624, val loss: 2.588\n",
      "step: 757, loss: 2.570, val loss: 2.579\n",
      "step: 758, loss: 2.589, val loss: 2.586\n",
      "step: 759, loss: 2.564, val loss: 2.589\n",
      "step: 760, loss: 2.565, val loss: 2.558\n",
      "step: 761, loss: 2.592, val loss: 2.569\n",
      "step: 762, loss: 2.633, val loss: 2.568\n",
      "step: 763, loss: 2.606, val loss: 2.574\n",
      "step: 764, loss: 2.550, val loss: 2.549\n",
      "step: 765, loss: 2.550, val loss: 2.571\n",
      "step: 766, loss: 2.588, val loss: 2.544\n",
      "step: 767, loss: 2.547, val loss: 2.589\n",
      "step: 768, loss: 2.565, val loss: 2.570\n",
      "step: 769, loss: 2.556, val loss: 2.566\n",
      "step: 770, loss: 2.563, val loss: 2.581\n",
      "step: 771, loss: 2.573, val loss: 2.558\n",
      "step: 772, loss: 2.566, val loss: 2.573\n",
      "step: 773, loss: 2.593, val loss: 2.592\n",
      "step: 774, loss: 2.556, val loss: 2.571\n",
      "step: 775, loss: 2.565, val loss: 2.623\n",
      "step: 776, loss: 2.561, val loss: 2.568\n",
      "step: 777, loss: 2.567, val loss: 2.563\n",
      "step: 778, loss: 2.544, val loss: 2.566\n",
      "step: 779, loss: 2.549, val loss: 2.564\n",
      "step: 780, loss: 2.579, val loss: 2.533\n",
      "step: 781, loss: 2.564, val loss: 2.555\n",
      "step: 782, loss: 2.561, val loss: 2.579\n",
      "step: 783, loss: 2.548, val loss: 2.587\n",
      "step: 784, loss: 2.562, val loss: 2.541\n",
      "step: 785, loss: 2.551, val loss: 2.573\n",
      "step: 786, loss: 2.581, val loss: 2.566\n",
      "step: 787, loss: 2.573, val loss: 2.545\n",
      "step: 788, loss: 2.572, val loss: 2.574\n",
      "step: 789, loss: 2.593, val loss: 2.622\n",
      "step: 790, loss: 2.568, val loss: 2.557\n",
      "step: 791, loss: 2.600, val loss: 2.637\n",
      "step: 792, loss: 2.543, val loss: 2.555\n",
      "step: 793, loss: 2.558, val loss: 2.568\n",
      "step: 794, loss: 2.562, val loss: 2.592\n",
      "step: 795, loss: 2.565, val loss: 2.617\n",
      "step: 796, loss: 2.544, val loss: 2.596\n",
      "step: 797, loss: 2.562, val loss: 2.576\n",
      "step: 798, loss: 2.583, val loss: 2.542\n",
      "step: 799, loss: 2.595, val loss: 2.526\n",
      "step: 800, loss: 2.542, val loss: 2.583\n",
      "step: 801, loss: 2.526, val loss: 2.562\n",
      "step: 802, loss: 2.552, val loss: 2.560\n",
      "step: 803, loss: 2.609, val loss: 2.585\n",
      "step: 804, loss: 2.558, val loss: 2.545\n",
      "step: 805, loss: 2.550, val loss: 2.575\n",
      "step: 806, loss: 2.580, val loss: 2.545\n",
      "step: 807, loss: 2.548, val loss: 2.594\n",
      "step: 808, loss: 2.579, val loss: 2.560\n",
      "step: 809, loss: 2.549, val loss: 2.527\n",
      "step: 810, loss: 2.531, val loss: 2.556\n",
      "step: 811, loss: 2.586, val loss: 2.567\n",
      "step: 812, loss: 2.560, val loss: 2.596\n",
      "step: 813, loss: 2.554, val loss: 2.568\n",
      "step: 814, loss: 2.570, val loss: 2.578\n",
      "step: 815, loss: 2.578, val loss: 2.536\n",
      "step: 816, loss: 2.586, val loss: 2.575\n",
      "step: 817, loss: 2.565, val loss: 2.567\n",
      "step: 818, loss: 2.547, val loss: 2.603\n",
      "step: 819, loss: 2.579, val loss: 2.524\n",
      "step: 820, loss: 2.606, val loss: 2.598\n",
      "step: 821, loss: 2.547, val loss: 2.563\n",
      "step: 822, loss: 2.560, val loss: 2.555\n",
      "step: 823, loss: 2.558, val loss: 2.542\n",
      "step: 824, loss: 2.543, val loss: 2.517\n",
      "step: 825, loss: 2.536, val loss: 2.575\n",
      "step: 826, loss: 2.554, val loss: 2.574\n",
      "step: 827, loss: 2.560, val loss: 2.583\n",
      "step: 828, loss: 2.559, val loss: 2.550\n",
      "step: 829, loss: 2.540, val loss: 2.543\n",
      "step: 830, loss: 2.583, val loss: 2.559\n",
      "step: 831, loss: 2.592, val loss: 2.548\n",
      "step: 832, loss: 2.549, val loss: 2.567\n",
      "step: 833, loss: 2.544, val loss: 2.544\n",
      "step: 834, loss: 2.563, val loss: 2.531\n",
      "step: 835, loss: 2.560, val loss: 2.568\n",
      "step: 836, loss: 2.530, val loss: 2.547\n",
      "step: 837, loss: 2.534, val loss: 2.537\n",
      "step: 838, loss: 2.551, val loss: 2.564\n",
      "step: 839, loss: 2.530, val loss: 2.540\n",
      "step: 840, loss: 2.560, val loss: 2.539\n",
      "step: 841, loss: 2.579, val loss: 2.603\n",
      "step: 842, loss: 2.535, val loss: 2.555\n",
      "step: 843, loss: 2.589, val loss: 2.562\n",
      "step: 844, loss: 2.586, val loss: 2.512\n",
      "step: 845, loss: 2.544, val loss: 2.538\n",
      "step: 846, loss: 2.562, val loss: 2.560\n",
      "step: 847, loss: 2.581, val loss: 2.545\n",
      "step: 848, loss: 2.538, val loss: 2.583\n",
      "step: 849, loss: 2.600, val loss: 2.558\n",
      "step: 850, loss: 2.558, val loss: 2.544\n",
      "step: 851, loss: 2.533, val loss: 2.534\n",
      "step: 852, loss: 2.562, val loss: 2.572\n",
      "step: 853, loss: 2.524, val loss: 2.554\n",
      "step: 854, loss: 2.561, val loss: 2.543\n",
      "step: 855, loss: 2.566, val loss: 2.530\n",
      "step: 856, loss: 2.562, val loss: 2.534\n",
      "step: 857, loss: 2.524, val loss: 2.553\n",
      "step: 858, loss: 2.549, val loss: 2.527\n",
      "step: 859, loss: 2.553, val loss: 2.598\n",
      "step: 860, loss: 2.590, val loss: 2.583\n",
      "step: 861, loss: 2.627, val loss: 2.535\n",
      "step: 862, loss: 2.570, val loss: 2.563\n",
      "step: 863, loss: 2.579, val loss: 2.552\n",
      "step: 864, loss: 2.539, val loss: 2.541\n",
      "step: 865, loss: 2.567, val loss: 2.595\n",
      "step: 866, loss: 2.539, val loss: 2.564\n",
      "step: 867, loss: 2.598, val loss: 2.536\n",
      "step: 868, loss: 2.532, val loss: 2.557\n",
      "step: 869, loss: 2.555, val loss: 2.586\n",
      "step: 870, loss: 2.561, val loss: 2.577\n",
      "step: 871, loss: 2.527, val loss: 2.541\n",
      "step: 872, loss: 2.568, val loss: 2.530\n",
      "step: 873, loss: 2.554, val loss: 2.575\n",
      "step: 874, loss: 2.530, val loss: 2.560\n",
      "step: 875, loss: 2.572, val loss: 2.534\n",
      "step: 876, loss: 2.533, val loss: 2.535\n",
      "step: 877, loss: 2.551, val loss: 2.514\n",
      "step: 878, loss: 2.536, val loss: 2.575\n",
      "step: 879, loss: 2.573, val loss: 2.536\n",
      "step: 880, loss: 2.612, val loss: 2.522\n",
      "step: 881, loss: 2.560, val loss: 2.542\n",
      "step: 882, loss: 2.567, val loss: 2.542\n",
      "step: 883, loss: 2.546, val loss: 2.515\n",
      "step: 884, loss: 2.570, val loss: 2.508\n",
      "step: 885, loss: 2.543, val loss: 2.538\n",
      "step: 886, loss: 2.556, val loss: 2.525\n",
      "step: 887, loss: 2.527, val loss: 2.548\n",
      "step: 888, loss: 2.555, val loss: 2.517\n",
      "step: 889, loss: 2.540, val loss: 2.570\n",
      "step: 890, loss: 2.553, val loss: 2.535\n",
      "step: 891, loss: 2.558, val loss: 2.563\n",
      "step: 892, loss: 2.529, val loss: 2.537\n",
      "step: 893, loss: 2.556, val loss: 2.554\n",
      "step: 894, loss: 2.536, val loss: 2.606\n",
      "step: 895, loss: 2.520, val loss: 2.592\n",
      "step: 896, loss: 2.539, val loss: 2.525\n",
      "step: 897, loss: 2.558, val loss: 2.571\n",
      "step: 898, loss: 2.533, val loss: 2.528\n",
      "step: 899, loss: 2.562, val loss: 2.598\n",
      "step: 900, loss: 2.551, val loss: 2.541\n",
      "step: 901, loss: 2.548, val loss: 2.569\n",
      "step: 902, loss: 2.562, val loss: 2.525\n",
      "step: 903, loss: 2.513, val loss: 2.500\n",
      "step: 904, loss: 2.546, val loss: 2.551\n",
      "step: 905, loss: 2.520, val loss: 2.544\n",
      "step: 906, loss: 2.562, val loss: 2.541\n",
      "step: 907, loss: 2.584, val loss: 2.537\n",
      "step: 908, loss: 2.551, val loss: 2.543\n",
      "step: 909, loss: 2.560, val loss: 2.557\n",
      "step: 910, loss: 2.516, val loss: 2.504\n",
      "step: 911, loss: 2.520, val loss: 2.560\n",
      "step: 912, loss: 2.554, val loss: 2.537\n",
      "step: 913, loss: 2.514, val loss: 2.534\n",
      "step: 914, loss: 2.536, val loss: 2.505\n",
      "step: 915, loss: 2.531, val loss: 2.510\n",
      "step: 916, loss: 2.525, val loss: 2.582\n",
      "step: 917, loss: 2.503, val loss: 2.529\n",
      "step: 918, loss: 2.580, val loss: 2.529\n",
      "step: 919, loss: 2.585, val loss: 2.531\n",
      "step: 920, loss: 2.542, val loss: 2.532\n",
      "step: 921, loss: 2.574, val loss: 2.516\n",
      "step: 922, loss: 2.542, val loss: 2.529\n",
      "step: 923, loss: 2.512, val loss: 2.531\n",
      "step: 924, loss: 2.522, val loss: 2.537\n",
      "step: 925, loss: 2.513, val loss: 2.527\n",
      "step: 926, loss: 2.563, val loss: 2.533\n",
      "step: 927, loss: 2.560, val loss: 2.554\n",
      "step: 928, loss: 2.567, val loss: 2.549\n",
      "step: 929, loss: 2.591, val loss: 2.555\n",
      "step: 930, loss: 2.509, val loss: 2.537\n",
      "step: 931, loss: 2.531, val loss: 2.556\n",
      "step: 932, loss: 2.526, val loss: 2.545\n",
      "step: 933, loss: 2.522, val loss: 2.526\n",
      "step: 934, loss: 2.595, val loss: 2.552\n",
      "step: 935, loss: 2.534, val loss: 2.530\n",
      "step: 936, loss: 2.579, val loss: 2.549\n",
      "step: 937, loss: 2.539, val loss: 2.534\n",
      "step: 938, loss: 2.517, val loss: 2.562\n",
      "step: 939, loss: 2.543, val loss: 2.577\n",
      "step: 940, loss: 2.549, val loss: 2.579\n",
      "step: 941, loss: 2.524, val loss: 2.567\n",
      "step: 942, loss: 2.526, val loss: 2.498\n",
      "step: 943, loss: 2.556, val loss: 2.546\n",
      "step: 944, loss: 2.541, val loss: 2.541\n",
      "step: 945, loss: 2.562, val loss: 2.589\n",
      "step: 946, loss: 2.583, val loss: 2.526\n",
      "step: 947, loss: 2.516, val loss: 2.552\n",
      "step: 948, loss: 2.580, val loss: 2.528\n",
      "step: 949, loss: 2.554, val loss: 2.541\n",
      "step: 950, loss: 2.566, val loss: 2.611\n",
      "step: 951, loss: 2.533, val loss: 2.529\n",
      "step: 952, loss: 2.508, val loss: 2.523\n",
      "step: 953, loss: 2.537, val loss: 2.532\n",
      "step: 954, loss: 2.535, val loss: 2.531\n",
      "step: 955, loss: 2.520, val loss: 2.601\n",
      "step: 956, loss: 2.536, val loss: 2.561\n",
      "step: 957, loss: 2.535, val loss: 2.534\n",
      "step: 958, loss: 2.571, val loss: 2.499\n",
      "step: 959, loss: 2.516, val loss: 2.526\n",
      "step: 960, loss: 2.530, val loss: 2.538\n",
      "step: 961, loss: 2.528, val loss: 2.566\n",
      "step: 962, loss: 2.524, val loss: 2.511\n",
      "step: 963, loss: 2.558, val loss: 2.573\n",
      "step: 964, loss: 2.508, val loss: 2.500\n",
      "step: 965, loss: 2.528, val loss: 2.534\n",
      "step: 966, loss: 2.568, val loss: 2.531\n",
      "step: 967, loss: 2.540, val loss: 2.510\n",
      "step: 968, loss: 2.580, val loss: 2.514\n",
      "step: 969, loss: 2.519, val loss: 2.517\n",
      "step: 970, loss: 2.553, val loss: 2.550\n",
      "step: 971, loss: 2.561, val loss: 2.575\n",
      "step: 972, loss: 2.523, val loss: 2.540\n",
      "step: 973, loss: 2.525, val loss: 2.561\n",
      "step: 974, loss: 2.521, val loss: 2.496\n",
      "step: 975, loss: 2.529, val loss: 2.543\n",
      "step: 976, loss: 2.526, val loss: 2.511\n",
      "step: 977, loss: 2.517, val loss: 2.532\n",
      "step: 978, loss: 2.516, val loss: 2.530\n",
      "step: 979, loss: 2.543, val loss: 2.558\n",
      "step: 980, loss: 2.534, val loss: 2.542\n",
      "step: 981, loss: 2.541, val loss: 2.517\n",
      "step: 982, loss: 2.505, val loss: 2.629\n",
      "step: 983, loss: 2.540, val loss: 2.512\n",
      "step: 984, loss: 2.557, val loss: 2.545\n",
      "step: 985, loss: 2.542, val loss: 2.516\n",
      "step: 986, loss: 2.542, val loss: 2.558\n",
      "step: 987, loss: 2.501, val loss: 2.544\n",
      "step: 988, loss: 2.571, val loss: 2.532\n",
      "step: 989, loss: 2.549, val loss: 2.513\n",
      "step: 990, loss: 2.566, val loss: 2.539\n",
      "step: 991, loss: 2.530, val loss: 2.541\n",
      "step: 992, loss: 2.537, val loss: 2.534\n",
      "step: 993, loss: 2.498, val loss: 2.538\n",
      "step: 994, loss: 2.521, val loss: 2.591\n",
      "step: 995, loss: 2.531, val loss: 2.530\n",
      "step: 996, loss: 2.511, val loss: 2.571\n",
      "step: 997, loss: 2.540, val loss: 2.500\n",
      "step: 998, loss: 2.530, val loss: 2.543\n",
      "step: 999, loss: 2.509, val loss: 2.546\n",
      "2.5530991554260254\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# Fix device and move model to it\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "m = model  # keep alias consistent\n",
    "\n",
    "# Fix get_batch (use block_size, not batch_size, and keep tensors on the same device as the model)\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    if len(data) <= block_size + 1:\n",
    "        raise ValueError(\"Random chunk too small for requested block_size.\")\n",
    "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + 1 + block_size] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    losses = estimate_loss()\n",
    "    print(f\"step: {iter}, loss: {losses['train'].item():.3f}, val loss: {losses['val'].item():.3f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "with open('model-01.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d70a8956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Where the hell are you?\" ormoorearit vad maydin?\"\n",
      "Jad7wmersilithawo\n",
      "\"I(oy sith bon aie oe.\n",
      "amy boulm tosid r, anouwory\n",
      "ithl\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello! Where the hell are you?\"\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea7c85e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! What can you do for me? \n",
      "foudsonitheargngedis. bu se the f outhastadenth to lsantw alerorgrtid acoe wcen pusflesen.\n",
      "caiwharit\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello! What can you do for me? \\n\"\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
